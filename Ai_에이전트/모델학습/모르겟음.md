좋아, PR 스타일 패치로 딱 핵심 3가지만 넣었어—Predict API + 모델 버저닝/심링크 + (옵션) 간단 설명 스텁.
아래 diff들 그대로 적용하면 바로 /predict API 쓸 수 있어.

⸻

#1 서버에 Predict 블루프린트 등록

*** a/server.py
--- b/server.py
@@
 from utils.model_trainer import train_model_from_logs
+from api_predict import bp as predict_bp
 
 app = Flask(__name__)
 
@@
 if __name__ == "__main__":
     print("🔧 서버 실행 중... http://0.0.0.0:3000")
     start_scheduler()
+    # 🔌 Predict API 등록
+    app.register_blueprint(predict_bp)
     app.run(host="0.0.0.0", port=3000)


⸻

#2 새 파일: api_predict.py (API 계약/검증 + 타깃 라우팅 + 불확실성)

*** /dev/null
--- b/api_predict.py
@@
+from flask import Blueprint, request, jsonify
+from pydantic import BaseModel, Field, ValidationError
+from typing import Optional, Literal
+import joblib, os
+from functools import lru_cache
+
+bp = Blueprint("predict", __name__)
+MODEL_DIR = "models"  # reward_latest.pkl, accuracy_latest.pkl 등 심링크 사용
+
+class PredictIn(BaseModel):
+    text: str = Field(..., min_length=5)
+    target: Literal["reward", "accuracy"] = "reward"
+    explain: bool = False
+
+class PredictOut(BaseModel):
+    target: str
+    prediction: float
+    proba: Optional[float] = None
+    lower: Optional[float] = None
+    upper: Optional[float] = None
+    model_version: str
+    explanation: Optional[str] = None
+
+@lru_cache(maxsize=8)
+def load_model(target: str):
+    link = os.path.join(MODEL_DIR, f"{target}_latest.pkl")
+    model = joblib.load(link)
+    version = os.path.realpath(link).split(os.sep)[-1]
+    return model, version
+
+def explain_prediction_stub(text: str, target: str, pred) -> str:
+    # 실제 서비스에선 GPT-5 설명 API로 교체
+    words = [w for w in text.split() if len(w) > 5]
+    key = ", ".join(sorted(set(words))[:5])
+    return f"[stub] {target}={pred:.3f}. 핵심토픽: {key}."
+
+@bp.route("/predict", methods=["POST"])
+def predict():
+    try:
+        payload = PredictIn(**request.get_json(force=True))
+    except ValidationError as e:
+        return jsonify({"error": e.errors()}), 400
+
+    model, version = load_model(payload.target)
+    pred = model.predict([payload.text])[0]
+
+    out = {
+        "target": payload.target,
+        "prediction": float(pred),
+        "model_version": version,
+    }
+
+    # 분류: 확률 제공
+    if hasattr(model, "predict_proba"):
+        proba = model.predict_proba([payload.text])[0]
+        if proba.shape[-1] == 2:
+            out["proba"] = float(proba[0, 1])
+    else:
+        # 회귀: 임시 PI (후에 캘리브레이션 교체)
+        out["lower"] = float(pred - 0.1)
+        out["upper"] = float(pred + 0.1)
+
+    if payload.explain:
+        out["explanation"] = explain_prediction_stub(payload.text, payload.target, pred)
+
+    return jsonify(PredictOut(**out).dict())


⸻

#3 모델 학습: 버저닝 + 최신 심링크 (reward 기준)

*** a/utils/model_trainer.py
--- b/utils/model_trainer.py
@@
 import joblib
 import os
 import json
+import time
 
 LOG_PATH = "logs/experiment_log.json"
 MODEL_PATH = "models/reward_predictor.pkl"
 
@@
 def train_model():
     df = load_logs()
     df = preprocess(df)
@@
     acc = model.score(X_test, y_test)
     print(f"✅ 모델 학습 완료 (정확도: {acc:.2f})")
 
-    os.makedirs("models", exist_ok=True)
-    joblib.dump(model, MODEL_PATH)
-    print(f"📦 모델 저장 완료: {MODEL_PATH}")
+    os.makedirs("models", exist_ok=True)
+    # 🆕 버저닝 파일 + latest 심링크 운용
+    ts = time.strftime("%Y%m%d-%H%M%S")
+    versioned = os.path.join("models", f"reward_cls_{ts}.pkl")
+    joblib.dump(model, versioned)
+    print(f"📦 모델 저장 완료: {versioned}")
+
+    latest = os.path.join("models", "reward_latest.pkl")
+    try:
+        if os.path.islink(latest) or os.path.exists(latest):
+            os.unlink(latest)
+        os.symlink(os.path.basename(versioned), latest)
+    except OSError:
+        # 윈도우/권한 이슈 대비: 하드카피 폴백
+        joblib.dump(model, latest)
+    print(f"🔗 최신 모델 심링크/복사 갱신: {latest}")
 
@@
 if __name__ == "__main__":
     train_model()

(참고) 회귀용 accuracy_latest.pkl을 따로 학습/저장하려면 유사 로직으로 accuracy_reg_{ts}.pkl 생성하고 accuracy_latest.pkl 심링크 갱신 함수 추가해주면 끝입니다.

⸻

#4 의존성 업데이트

*** a/requirements.txt
--- b/requirements.txt
@@
 flask
 scikit-learn
 joblib
 pandas
+pydantic>=2
 apscheduler


⸻

사용법 스모크 테스트

# 1) 학습 (reward 이진분류 가정)
curl -X POST http://localhost:3000/train

# 2) 예측
curl -X POST http://localhost:3000/predict \
  -H "Content-Type: application/json" \
  -d '{"text":"새로운 RL 아이디어와 실험 요약...", "target":"reward", "explain":true}'

예상 응답

{
  "target": "reward",
  "prediction": 1.0,
  "proba": 0.82,
  "model_version": "reward_cls_20250802-153012.pkl",
  "explanation": "[stub] reward=1.000. 핵심토픽: ..."
}


⸻

이번 PR 핵심 3가지 (딱 의미 있는 것만)
	1.	/predict API: 스키마 검증(Pydantic) + 타깃 라우팅 + 불확실성 반환
	2.	모델 버저닝/심링크: 운영 롤백/추적 쉬움 (*_latest.pkl)
	3.	설명 스텁: GPT‑5 설명 연동 자리 확보(지금은 경량 스텁)

필요하면 회귀 타깃(accuracy) 학습 루틴도 같은 방식으로 바로 추가해줄게.