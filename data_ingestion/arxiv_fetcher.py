import requests
from utils.paper_fetcher import fetch_arxiv_papers

import os
import json

# ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ URL ì •ì˜ (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
PORT = os.getenv("PORT", 3000)
BASE_URL = f"http://localhost:{PORT}"
INGEST_URL = f"{BASE_URL}/ingest"
CHECK_DUPLICATES_URL = f"{BASE_URL}/check_duplicates"

# ë‹¤ì–‘í•œ AI ê´€ë ¨ ê²€ìƒ‰ í‚¤ì›Œë“œ
SEARCH_KEYWORDS = [
    "reinforcement learning",
    "deep learning",
    "neural networks",
    "computer vision",
    "natural language processing",
    "transformer models",
    "generative AI",
    "machine learning optimization",
    "graph neural networks",
    "meta learning"
]

KEYWORD_INDEX_FILE = ".keyword_index.json"

def get_next_keyword():
    """í‚¤ì›Œë“œ ìˆœí™˜: ë§¤ë²ˆ ë‹¤ë¥¸ í‚¤ì›Œë“œ ë°˜í™˜"""
    if os.path.exists(KEYWORD_INDEX_FILE):
        try:
            with open(KEYWORD_INDEX_FILE, "r") as f:
                data = json.load(f)
                current_index = data.get("index", 0)
        except:
            current_index = 0
    else:
        current_index = 0
    
    # ë‹¤ìŒ í‚¤ì›Œë“œ ì„ íƒ
    keyword = SEARCH_KEYWORDS[current_index % len(SEARCH_KEYWORDS)]
    next_index = (current_index + 1) % len(SEARCH_KEYWORDS)
    
    # ì¸ë±ìŠ¤ ì €ì¥
    with open(KEYWORD_INDEX_FILE, "w") as f:
        json.dump({"index": next_index}, f)
    
    return keyword

def collect_and_ingest(query=None, max_results=30):
    """
    ì§€ì •ëœ ì¿¼ë¦¬ë¡œ ArXivì—ì„œ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³ , ì¤‘ë³µì„ í™•ì¸í•œ ë’¤ ìƒˆ ë…¼ë¬¸ë§Œ ì„œë²„ì˜ /ingest ì—”ë“œí¬ì¸íŠ¸ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    queryê°€ Noneì´ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ í‚¤ì›Œë“œ ì„ íƒ
    """
    if query is None:
        query = get_next_keyword()
    
    print(f"ğŸ” '{query}' ê´€ë ¨ ìµœì‹  ë…¼ë¬¸ {max_results}ê°œ ìˆ˜ì§‘ ì‹œì‘...")
    try:
        papers = fetch_arxiv_papers(query, max_results=max_results)
        if not papers:
            print("ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
    except Exception as e:
        print(f"ğŸ”¥ ArXivì—ì„œ ë…¼ë¬¸ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    paper_titles = [paper['title'] for paper in papers]
    print(f"âœ… {len(papers)}ê°œ ë…¼ë¬¸ í›„ë³´ í™•ì¸ ì™„ë£Œ. ì„œë²„ì— ì¤‘ë³µ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

    # ì„œë²„ì— ì¤‘ë³µ í™•ì¸ ìš”ì²­
    try:
        response = requests.post(CHECK_DUPLICATES_URL, json={"titles": paper_titles})
        response.raise_for_status()
        duplicates = set(response.json().get("duplicates", []))
        print(f"â„¹ï¸ {len(duplicates)}ê°œì˜ ì¤‘ë³µ ë…¼ë¬¸ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
    except requests.exceptions.RequestException as e:
        print(f"ğŸ”¥ ì„œë²„ì™€ í†µì‹  ì‹¤íŒ¨ (ì¤‘ë³µ í™•ì¸): {e}")
        return

    # ìƒˆ ë…¼ë¬¸ë§Œ í•„í„°ë§
    new_papers = [paper for paper in papers if paper['title'] not in duplicates]

    if not new_papers:
        print("âœ… ìƒˆë¡œìš´ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ë…¼ë¬¸ì´ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")
        return

    print(f"ğŸ“¤ {len(new_papers)}ê°œì˜ ìƒˆë¡œìš´ ë…¼ë¬¸ì„ ì„œë²„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤...")

    for paper in new_papers:
        # ì„œë²„ë¡œ ì „ì†¡í•  ìƒì„¸ ë°ì´í„° êµ¬ì¡°í™”
        payload = {
            "title": paper["title"],
            "summary": paper["summary"],
            "authors": [author.name for author in paper.get("authors", [])], # Example of richer data
            "pdf_url": paper["pdf_url"],
            "source": "collector_v2",
            "query": query,
            # ì‹œë®¬ë ˆì´ì…˜ëœ í•„ë“œ (í”„ë¡œí† íƒ€ì… ì•„ì´ë””ì–´ í™•ì¥)
            "idea": "ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ì˜ íƒí—˜ ì „ëµ ê°œì„ ",
            "keywords": ["Reinforcement Learning", "Exploration", "Novelty"],
            "label": 1 # ê¸°ë³¸ ë ˆì´ë¸”
        }

        try:
            response = requests.post(INGEST_URL, json=payload)
            response.raise_for_status()
            print(f"  - ì „ì†¡ ì„±ê³µ: \"{paper['title'][:40]}...\"")
        except requests.exceptions.RequestException as e:
            print(f"ğŸ”¥ ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨: {e}")
            print(f"  - ì‹¤íŒ¨í•œ ë…¼ë¬¸: \"{paper['title'][:40]}...\"")

if __name__ == "__main__":
    # ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì§ì ‘ ì‹¤í–‰í•  ë•Œ ì´ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
    collect_and_ingest()
