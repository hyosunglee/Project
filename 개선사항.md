## 작업량 분석 (GI와 MDA 혼합)

우선 각 개선 항목이 요구하는 관찰(Observation), 연결(Connection), 패턴 인식(Pattern), 종합적 사고(Synthesis) 수준과 고정관념/편향(Assumption/Bias)을 평가하여 **천재적 통찰 지수(GI)**를 추정하고, 동시에 **다차원적 분석(MDA)** 프레임워크를 활용해 시간적·공간적·추상적 차원에서 필요 자원을 살펴봅니다.

* **다양한 라벨 확보**는 **관찰과 패턴 인식** 측면이 높습니다. 이미 수집된 데이터는 긍정(1) 라벨로 치우쳐 있어 불균형하며, Balanced dataset을 확보하려면 추가적인 부정(0) 라벨이 필요합니다. **라벨링 작업이 단순 반복적이라도 사람의 판단이 핵심**이며, 단순 텍스트 태깅조차 한 건당 0.03~0.05달러 정도 비용이 발생합니다. 수백만 건 이상 라벨이 필요할 경우 비용과 시간이 기하급수적으로 증가하므로 **A+B(고정관념과 편향)** 요소가 낮아야 하고, **S(종합)** 능력이 높아야 효율적으로 진행됩니다.
* **요약 품질 향상**은 **연결과 종합** 요소가 높으며, 기존 ArXiv 제공 요약을 그대로 사용하는 것에서 탈피해 핵심 문장 추출, 불용어 제거, 추상적 요약까지 적용해야 합니다. AI 요약은 빠르지만 **대규모 모델을 학습하고 운영하기 위해 상당한 계산 자원이 필요**하며, 수동 요약은 몇 주에서 몇 달이 걸릴 수 있습니다. MDA 관점에서 **시간적 차원**에서 장기적 투자, **공간적 차원**에서는 글로벌 데이터 접근 및 컴퓨팅 인프라가 필요합니다.
* **재학습 대상 라벨링**은 loop_logic()에서 confidence가 낮은 샘플을 모아 재학습을 트리거하지만, 이 데이터를 사람이 직접 라벨링해야 합니다. 이는 데이터 라벨 확보와 마찬가지로 **높은 인건비와 시간이 필요**하며, 작업에 참여한 여러 평가자 간 의견 차이가 발생할 수 있고, **사람이 주관적인 평가를 내릴 때 시간과 비용이 많이 든다**는 점이 연구에서도 강조됩니다.
* **로그 파일 경로 혼선**과 **전문가 리뷰**는 상대적으로 작업량이 적습니다. 로그 경로 통일은 코드 수정으로 빠르게 해결할 수 있고, 전문가 리뷰는 주기적으로 모델 성능을 검토하며 피드백을 제공하는 수준이지만 대규모 데이터 라벨링만큼 노동집약적이지는 않습니다. 전문가 평가도 사람에 따라 주관이 달라서 시간·비용이 들지만 주기성을 조정하면 부담을 줄일 수 있습니다.

GI 수식에 따라 **관찰(높음), 연결(중), 패턴 인식(중), 종합(높음)** 그리고 **고정관념과 편향(낮음)**을 고려하면, **재학습 대상 라벨링**과 **다양한 라벨 확보**가 가장 높은 GI를 필요로 하고, 동시에 MDA의 모든 차원에 걸쳐 큰 자원(시간·인력·비용)을 요구하므로 **작업량이 가장 많을 것**으로 판단됩니다. 요약 품질 향상은 계산 자원과 연구개발 노력이 요구되지만 자동화 가능성이 있어 인력 부담은 상대적으로 낮습니다. 로그 경로 정비와 전문가 리뷰는 단기간 또는 주기적으로 진행되는 관리 업무에 가까워 작업량이 적습니다.

## 창의적 아이디어 10개 이상 (3000자 이상)

1. **능동 학습(Active Learning)과 크라우드소싱 결합**: 단순히 `/seed`로 더미 데이터를 생성하는 대신, 현재 모델이 **가장 혼란스러워하는 데이터**만 선별하여 크라우드소싱 플랫폼이나 내부 평가자에게 라벨링을 요청하는 방식을 적용한다. 이는 라벨링 전체 비용을 대폭 줄여주고, 모델 개선 속도를 높인다. 이미 일부 연구에서는 간단한 라벨 작업의 비용이 건당 0.03~0.05달러 수준임을 지적하며, 전체 데이터에 일일이 라벨을 붙이는 것은 비효율적이다. Active Learning은 **선택적 라벨링**을 통해 예산을 효율적으로 사용하며, 크라우드 라벨링 시 라벨 불일치 여부를 교차 검증하여 **Assumption**과 **Bias**를 최소화할 수 있다.
2. **자동화된 요약 파이프라인 구축**: ArXiv의 요약을 그대로 사용하는 대신, 최신 Transformer 기반 추상 요약 모델을 도입해 수집된 논문의 텍스트를 요약한다. 논문들의 구조(제목·초록·도입·결론)를 분석해 **중요 문장 추출**과 **불용어 제거**를 수행하고, 이후 **BERTSum**이나 **PEGASUS** 같은 사전학습 모델로 요약을 생성한다. Sourcely의 분석에 따르면 AI 요약은 수천 편의 논문을 **몇 분 만에 처리**하지만 시스템 구축 초기에는 큰 계산 자원이 필요하다. 이를 위해 오픈소스 요약 라이브러리와 클라우드 GPU 인스턴스를 활용하고, 연속 학습을 통해 요약 품질을 개선한다.
3. **라벨 자동 추정 및 인간 검수**: confidence가 낮은 샘플을 재학습 버퍼에 저장할 때, 임시 예측 라벨을 부여하고 해당 샘플을 사용자에게 제시하여 **맞는지 여부만 확인**하도록 한다. 이는 완전한 라벨링보다 간단한 의사결정이므로 시간과 비용을 절감한다. 예를 들어 "이 논문은 긍정적인가요?" 같은 간단한 질문에 '예/아니오'를 선택하는 방식이다. 이는 AIMultiple 분석에서 언급된 것처럼 **사람 평가가 시간과 비용을 많이 소모**한다는 문제를 완화해 준다.
4. **노이즈 대조 학습을 통한 반라벨 생성**: 수집된 모든 논문을 긍정(1)로 라벨링하는 대신, 별도의 규칙 기반 스코어링(예: 키워드 부재, 특정 영역 비포함 등)을 활용해 **잠재적 부정 샘플**을 자동 생성한다. 이는 라벨 불균형을 해소하고, 모델이 더 다양한 분포를 학습하도록 도울 수 있다. 뿐만 아니라, self-supervised 학습 기법을 도입하여 아예 라벨 없이 의미있는 표현을 학습한 뒤 소수의 라벨만으로 파인튜닝하는 접근도 가능하다.
5. **후속 재학습 로직 개선**: 현재 loop_logic()는 confidence가 낮은 샘플이 10개 이상일 때 재학습을 실행한다. 그러나 confidence는 논문 요약의 길이나 난이도에 따라 왜곡될 수 있다. 이를 해결하기 위해 **confidence 분포를 분석해 동적 임계값**을 도입하거나, **entropy** 기반 불확실성 척도를 추가해 검증한다. 이와 함께 재학습 시 기존 모델과 앙상블을 구성하여 과적합을 방지할 수 있다.
6. **전문가 리뷰 워크플로우 자동화**: 모델 성능 검증에 도메인 전문가가 필요하지만 이는 비용이 많이 든다. AIMultiple의 연구는 **사람 평가가 주관적이고 비용이 많이 들며 시간이 오래 걸린다**고 지적한다. 따라서 전문가 리뷰를 정기적인 주기로만 진행하고, 나머지 평가에는 자동화된 메트릭을 활용한다. 예를 들어 모델이 추천한 논문 Top-N과 실제 인용지수·독창성 점수 간의 상관을 계산하거나, 기존 벤치마크 데이터셋으로 정확도를 측정하는 방식이다. 전문가 리뷰는 **이상 징후가 발견될 때만** 개입하도록 하여 부담을 줄인다.
7. **지속적인 데이터 파이프라인 검증을 위한 MLOps 도입**: 로그 파일 경로 혼선 문제는 작은 일이지만 시스템 전체 안정성에 영향을 줄 수 있다. 모든 파일 경로와 결과 저장 위치를 중앙 설정 파일(`config.yaml`)에서 관리하도록 바꾸고, 데이터 검증 스크립트를 통해 잘못된 경로나 파일 포맷을 자동으로 감지한다. 이는 복잡한 시스템을 하위 구성 요소로 분해하고 최적화하는 **복잡성 해결 매트릭스**를 적용한 것으로, 오류 탐지를 자동화해 개발자 부담을 줄인다.
8. **커뮤니티와 협력하는 라벨링 게임화 플랫폼 개발**: 학습 데이터 라벨링의 비용을 줄이기 위해 **게임화된 라벨링 플랫폼**을 만들어 사용자들이 재미를 느끼면서 라벨 작업에 참여하도록 한다. 예를 들어, 요약된 논문에 ‘추천할 만하다/덜 추천된다’ 등을 선택하고 점수를 획득하는 간단한 게임 구조를 도입한다. 이는 Crowdsourcing과 같은 구조지만 참여자의 동기를 상승시켜 대규모 데이터를 빠르게 라벨링할 수 있다.
9. **요약 품질 향상 연구와 실험 공유**: 요약 알고리즘 개선 과정에서 다양한 모델(추출적 vs 추상적 vs 혼합 모델) 및 전처리 기법을 실험하고, 그 결과를 오픈커뮤니티에 공유해 피드백을 받는다. 이렇게 하면 더 넓은 관점의 의견을 들을 수 있어 GI 공식의 **연결과 종합** 점수가 높아지고, 편향을 줄일 수 있다. 논문 제목·초록·저자 정보를 활용한 메타데이터 기반 요약도 시험해볼 수 있다.
10. **모듈화된 코드 구조와 학습 파이프라인 재설계**: 현재 프로젝트에는 `utils/model_trainer.py`와 `utils/trainer.py` 등 비슷한 이름의 스크립트들이 공존하며 경로 혼선 위험이 있다. 이를 해결하기 위해 모든 학습 관련 함수를 하나의 모듈로 통합하고, 입력 데이터 경로·모델 저장 경로·결과 저장 경로를 **환경변수 또는 설정파일에서 읽어오는 방식**으로 재설계한다. 이렇게 하면 앞으로 기능을 추가하더라도 혼선이 없다.
11. **멀티태스킹 모델과 전이학습 도입**: 단순 로지스틱 회귀 대신 최신 Transformer 기반 분류 모델을 도입하고, 요약된 텍스트를 사용해 **보상 모델뿐 아니라 난이도 예측, 분야 분류 등 여러 태스크를 동시에 학습**하는 멀티태스킹 학습을 시도한다. 이렇게 하면 데이터 효율성이 높아지고, 한정된 라벨로도 다양한 문제를 해결할 수 있다.
12. **인과적 차원 분석을 통한 오류 원인 규명**: 모델이 잘못된 예측을 했을 때, 단순히 결과만 보는 것이 아니라 **인과적 분석(원인-과정-결과)**을 통해 왜 그런 판단을 내렸는지를 탐구한다. 예를 들어 텍스트 내 특정 키워드가 과도하게 높은 가중치를 가질 경우, 데이터 전처리나 모델 설계를 조정해 편향을 줄인다. 이는 인간 전문가의 설명 가능성 요구를 만족시키고, 재학습을 위한 더 정확한 피드백을 제공한다.
13. **윤리적 검토 및 책임 있는 AI 적용**: 도메인 전문가 리뷰 외에도 윤리·법률 전문가가 시스템 설계에 참여하여 데이터 수집과 라벨링이 저작권, 개인정보보호, 공정성 측면에서 문제가 없는지 검토한다. 이는 미래의 법적 위험을 줄이고, 모델 신뢰도를 높이는 데 기여한다.

### 결론

위의 분석을 종합하면, **라벨 수집 및 재학습 대상 데이터의 수동 라벨링** 작업이 가장 큰 작업량을 요구합니다. 이는 단위당 비용과 시간이 높은 수작업이며, **사람 평가가 주관적이고 비용이 많이 드는 평가 과정**이라는 연구 결과로 뒷받침됩니다. 요약 품질 향상은 알고리즘 개발과 계산 자원이 많이 필요하지만, 자동화 가능성이 높아 반복적인 인력 투입이 적습니다. 로그 경로 통일과 전문가 리뷰는 상대적으로 간단한 관리 및 검토 작업으로 분류됩니다.
