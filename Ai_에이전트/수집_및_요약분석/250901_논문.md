###2024-2025년 최신 컴퓨터 비전 SOTA 기술 동향

최근 1년간(2024년 하반기부터 2025년 현재) 컴퓨터 비전 분야에서는 생성 모델의 발전, 멀티모달 융합, 3D 비전 기법의 고도화, 효율적인 모델 아키텍처 등의 흐름이 두드러졌습니다. 특히 확산 모델(diffusion model) 기반 생성 기술이 각종 비전 작업의 판도를 바꾸고 있고, 대규모 비전-언어 멀티모달 모델이 LLM과 결합되어 새로운 패러다임을 만들어내는 중입니다. 또한 NeRF로 대표되는 3D 신경렌더링 기법이 정교해지며, 경량화·효율화된 모델 구조에 대한 연구도 활발합니다. 본 문서에서는 2024년 후반부터 2025년 사이 주요 학회(CVPR, ICCV, ECCV, NeurIPS 등)와 arXiv에 발표된 SOTA 수준의 핵심 논문들을 중심으로, 각 기술의 모델 구조, 사용된 기법, 벤치마크 성능, 실제 적용 가능성을 중점적으로 분석합니다. 아래 표는 이 기간 등장한 대표적인 SOTA 논문들을 요약한 것입니다.

논문 제목 (발표 연도)	요약	모델/기법 특징	성능 지표 및 성과	관련 링크
Analyzing and Improving the Training Dynamics of Diffusion Models (CVPR 2024)  T. Karras et al.	확산 모델 학습 난조의 원인을 분석하고 네트워크 층 구성과 매개변수 정규화를 개선하여 동일 복잡도에서 이미지 품질 향상을 달성. 고급 구조 변경 없이도 훈련 안정화 및 성능 개선 가능성을 입증.	ADM 기반 U-Net 확산 모델의 층 구성 리디자인 (활성값/가중치 규모 유지)  EMA 파라미터 사후 조정 기법 제안	ImageNet-512 생성에서 FID 2.41 → 1.81로 개선 (동일 계산 비용에서) ￼. 기존 최고 성능을 모델 크기 1/5 수준으로 달성 ￼.	【31†openaccess】
InstanceDiffusion: Instance-level Control for Image Generation (CVPR 2024)  X. Wang et al.	텍스트-투-이미지 확산 모델에 개별 객체 단위 제어를 도입한 최신 연구. 프롬프트 내 각 객체마다 위치(bbox, 마스크 등)와 텍스트를 지정하면 정교한 배치 제어 이미지 생성이 가능.	UniFusion 블록으로 객체별 조건 주입, ScaleU로 해상도 스케일 조정, Multi-Instance Sampler로 다객체 생성 품질 향상	COCO 기반 평가에서, Bounding Box 조건 입력 시 이전 SOTA 대비 AP50 20.4%p 개선, 마스크 조건에서는 IoU 25.4%p 향상 ￼ (객체 위치 조건별 생성정합도 크게 향상).	【34†openaccess】
DeepCache: Fast Diffusion Model Inference (CVPR 2024)  Z. Xu et al.	디퓨전 모델 추론 가속 기법으로, U-Net의 고수준 특징을 캐시하여 인접 확산 단계에서 재활용함으로써 생성 속도를 크게 향상. 기존 확산 모델의 다단계 계산 중 중복 연산을 제거하는 접근.	U-Net 듀얼 브랜치 구성 (메인 브랜치 고수준 특징 + 스킵 브랜치 저수준 특징)  인접 단계 캐싱으로 중복계산 절약	Stable Diffusion v1.5에서 2.3배 빠른 이미지 생성, 대형 LDM-4-G 모델에서 4.1배 속도 향상 ￼. 이미지 품질 저하 없이 거의 손실없는 속도 개선 달성.	【8†기사】
InternVL: Scaling Up Vision-Language Foundation Models (CVPR 2024)  Z. Chen et al.	대규모 비전-언어 모델로, 비전 인코더를 60억 파라미터 규모로 확장하고 LLM과 정교하게 결합·정렬한 멀티모달 기반 모델. 웹 이미지-텍스트 데이터로 대규모 학습하여 다양한 시각-언어 과제에서 SOTA 성능 달성.	InternViT-6B 거대 비전 트랜스포머 + 기존 LLM 결합  대규모 대비학습으로 영상-텍스트 임베딩 정렬  LLM과 QLoRA 등 미들웨어로 통합 대화 가능	이미지 분류, 객체 인식, 영상 QA, 이미지/영상-텍스트 검색 등 32개 벤치마크에서 기존 최고 성능 달성 ￼. 개방형 모델 중 최고 수준의 시각 언어 이해력을 입증.	【18†openaccess】
ZeroNVS: Zero‑Shot 360° View Synthesis from a Single Image (CVPR 2024)  K. Sargent et al.	단일 이미지로부터 360도 시야의 새로운 뷰를 생성하는 3D-aware 확산 모델. 복잡한 다객체 장면에서도 동작하며, 6-DoF 카메라 조건 표현과 SDS 앵커링 기법으로 배경 다양성을 높여 현실감 있는 뷰 합성을 달성.	3D SDS(Score Distillation Sampling) 기반 단일 이미지 NeRF 생성  카메라 6자유도(6DoF) 조건 도입  다양한 뷰 앵커링으로 모드 붕괴 방지	DTU, Mip-NeRF 360 등 단일 이미지 뷰 합성 벤치마크에서 SOTA 성능 입증 ￼. 기존 기법이 어려운 실내/실외 복잡 장면까지 현실적으로 재구성.	【29†openaccess】
Wonder3D: Single Image to 3D using Cross-Domain Diffusion (CVPR 2024)  X. Long et al.	단일 2D 이미지로 고품질 3D 메시(mesh) 생성을 실현한 효율적 기법. 입력 이미지로부터 다시점 노말맵을 확산모델로 생성하고, 이를 geometry-aware 융합하여 세밀한 3D 형상을 복원.	크로스도메인 확산으로 노말맵 예측  geometry-aware normal fusion 알고리즘으로 메시 재구성  메쉬 텍스처 고해상도 생성	단일 이미지로 고해상도 텍스처 메쉬를 2~3분 내 생성 ￼. 기존 단일 영상 3D 복원법 대비 지오메트리 세부묘사와 효율성에서 큰 개선을 보임.	【36†openaccess】
EfficientViM: Efficient Vision Mamba with Hidden State Mixer (CVPR 2025)  S. Lee et al.	토큰-레벨 전역 표현을 효율적으로 처리하는 새로운 비전 백본. 컨볼루션+어텐션 기반 경량 모델에 **상태공간모델(SSM)**을 접목하여, 적은 연산으로 전역 의존성을 캡처. 은닉 상태 믹서와 다단계 융합으로 메모리 병목도 완화.	HSM-SSD 레이어: 숨은상태기반 채널믹싱으로 SSM 속도 향상  멀티스테이지 hidden state fusion으로 표현력 향상  Conv+Attention+SSM 하이브리드 경량 아키텍처	ImageNet-1k 분류에서 최고 효율 대비 0.7%p 높은 Top-1 정확도를 더 빠른 속도로 달성 ￼. 이미지 해상도 증가나 지식증류 활용 시도에서도 기존 경량 모델 대비 높은 처리량과 정확도 기록.	【37†openaccess】
Efficient Deformable ConvNets (DCNv4) (CVPR 2024)  Y. Xiong et al.	변형 합성곱 연산자의 최신 버전으로, DCNv3의 성능 및 속도 한계를 극복. Softmax 정규화를 제거해 동적 범위를 확보하고, 메모리 접근 최적화로 중복 계산 최소화. 다양한 비전 작업에서 ConvNeXt-Transformer 하이브리드를 능가하는 효율성과 성능을 달성.	Softmax 제거로 가중치 중요도 재분배 (동적 표현력 ↑)  메모리 패턴 최적화로 연산 낭비 감소  InternImage 등 최신 ConvNet 백본에 통합 용이	이전 버전 대비 전방향 연산 속도 3배 이상 향상 ￼. InternImage 백본에 DCNv4 적용 시 최대 80%↑ 속도 향상 및 추가 정확도 상승 ￼. 이미지 분류, 검출, 확산 U-Net 생성 등 광범위한 작업에서 기존 대비 성능 향상 입증.	【25†openaccess】

위 표에 정리된 논문들을 바탕으로, 각 주요 기술 트렌드를 항목별로 자세히 분석합니다. 생성 모델과 확산 기법의 발전, 멀티모달 비전-언어 모델, 3D 비전 및 NeRF 혁신, 그리고 효율적인 네트워크 아키텍처 분야로 나누어 살펴보겠습니다.

###확산 모델 기반 생성 기술의 발전

최근 컴퓨터 비전에서 **확산 모델(Diffusion Model)**이 이미지 생성 분야를 주도적 패러다임으로 이끌었습니다. 2024년 이후 많은 연구들이 확산 모델의 제어력 향상, 학습 효율 개선, 생성 속도 가속 등에 집중하고 있습니다. 대표적으로 StyleGAN 시리즈를 개발한 Tero Karras의 연구 ￼에서는 확산 모델 학습 시 발생하는 훈련 동역학의 불균형 문제를 분석하고, 네트워크 층 설계를 재구성하여 동일한 모델 복잡도에서 더 나은 이미지 품질을 달성했습니다. 이 연구는 기존 최고 성능을 훨씬 작은 모델로도 구현함으로써(예: ImageNet-512 생성 FID를 2.41에서 1.81로 개선 ￼) 확산 모델의 효율적 학습 가능성을 입증했습니다. 즉, 활성값과 가중치의 분포를 안정화하는 설계만으로도 5배 작은 모델로 이전 SOTA 성능을 달성해, 대규모 모델에 의존하지 않고도 최고 성능을 낼 길을 제시하였습니다 ￼. 이는 실제로 훈련 비용 절감과 모델 경량화에 기여하여, 확산 기반 생성 기술의 실제 적용 가능성을 높이는 의미있는 개선입니다.

한편, 확산 모델의 세밀한 제어와 다양한 활용을 위한 연구들도 두드러집니다. 과거에는 텍스트 조건으로 이미지를 통제하는 수준이었지만, **InstanceDiffusion (CVPR 2024)**은 여러 객체의 위치와 속성을 세밀히 지정하여 복잡한 장면 생성을 가능케 했습니다. 이 방법은 이미지 내 각 인스턴스마다 별도 텍스트 조건과 위치 지정(점 하나, 박스 영역, 분할 마스크 등)을 받아들여, 생성 결과에서 개별 객체들이 원하는 위치와 형태로 표현되도록 합니다. 예를 들어 텍스트 프롬프트와 함께 객체의 바운딩 박스를 제시하면 해당 영역에 해당 텍스트에 부합하는 객체를 정확히 생성해냅니다. 이를 위해 UniFusion이라는 새로운 모듈을 통해 객체별 임베딩을 확산 모델에 융합하고, ScaleU 블록으로 여러 객체 생성 시 해상도 균형을 조정하며, Multi-instance Sampler로 여러 객체 간 맥락 조정을 수행합니다 ￼. 그 결과 InstanceDiffusion은 객체 위치 조건별 생성 품질에서 기존 특수화 모델들을 크게 앞서는 성과를 냈습니다. 예를 들어 COCO 데이터셋 기반 평가에서, 바운딩박스 제시 조건의 경우 20.4%p, 분할 마스크 제시의 경우 25.4%p 이전 최고 기법보다 정확도 향상을 보였다고 보고되었습니다 ￼. 이는 사용자가 원하는 레이아웃대로 이미지 생성을 할 수 있음을 의미하며, 디자인, 콘텐츠 제작 분야에서 실용적으로 활용될 수 있는 큰 진전입니다.

확산 모델의 느린 생성 속도를 개선하는 연구도 활발합니다. 확산 모델은 다수의 단계별 노이즈 제거 연산으로 인해 추론 시간이 긴 문제가 있는데, 이를 해결하고자 추론 과정 최적화 기법들이 등장했습니다. **DeepCache (CVPR 2024)**는 U-Net 기반 확산 모델의 고수준 특징맵을 캐싱하여 인접 단계에서 재사용함으로써 중복 연산을 제거하는 방법을 제안했습니다 ￼. 구체적으로, U-Net의 메인 경로에서 얻어지는 상위 계층 특징들은 연속된 확산 타임스텝에서 큰 변화가 없다는 점에 착안하여, 한 스텝에서 계산한 특징을 다음 스텝에서 재활용하는 것입니다. 이를 통해 Stable Diffusion 1.5에서 2.3배, 대형 확산 모델(LDM-4)에서 4배 이상의 속도 향상을 이루었으며 ￼, 출력 품질에는 거의 영향을 주지 않아 사실상 손실 없는 가속을 달성했습니다. 이같은 개선은 실시간 이미지 생성이나 인터랙티브한 생성 응용에 확산 모델을 적용하는 데 필수적인 전진으로, 고품질 결과를 더 빠르게 얻음으로써 실제 서비스나 애플리케이션에 투입하기 용이해집니다.

비슷한 맥락에서, Qualcomm 연구진은 Clockwork Diffusion이라는 기법으로 확산 모델 추론의 비효율을 줄이는 시도를 선보였습니다. 이들은 모든 U-Net 레이어가 매 스텝 동일한 빈도로 계산될 필요가 없음을 지적하며, 저해상도 특징맵 부분은 몇 스텝 건너뛰어 재사용하는 전략과 **모델 경량화(distillation)**를 결합했습니다 ￼ ￼. 이를 통해 일부 이미지 편집 및 생성 작업에서 30% 이상의 연산량 절감을 이루고, FID 등 품질 지표에서 손실 없이 성능을 유지했습니다 ￼. 이러한 부분적인 계산 재사용 아이디어는 앞선 DeepCache와 유사한 맥락이지만, 모델 구조 자체를 여러 시간대(clock)로 분할한 점이 특이합니다. 즉, 고주파수로 업데이트가 필요한 층과 저주파수로 업데이트해도 되는 층을 나누어 ‘시계태엽’처럼 단계별로 동작시키는 개념입니다. 이는 경량 디바이스에서의 실시간 텍스트-이미지 생성 등 엣지 컴퓨팅 활용에도 유용한 방향입니다.

또 다른 방향으로, 영상(Video) 생성 및 편집에 확산 모델을 확장한 연구들도 주목받고 있습니다. 예컨대 **RAVE (CVPR 2024)**는 Zero-shot 영상 편집 방법으로, 입력 영상과 원하는 편집 텍스트(prompt)를 넣으면 영상의 모션은 보존하면서 내용만 바꾸는 결과 영상을 생성합니다 ￼ ￼. 이 모델은 ControlNet 구조를 응용하여, 영상의 프레임별 조건을 추출한 뒤 확산 모델의 노이즈 주입 순서를 임의로 섞는 기법(랜덤 노이즈 셔플링)을 통해 시간적 일관성을 유지하는 전략을 사용합니다 ￼ ￼. 그 결과, 원본 영상의 움직임이나 장면 구도는 유지하면서도 사용자가 지시한 대로 영상의 속성이나 배경을 변화시키는 놀라운 편집 능력을 보였습니다. 이는 향후 동영상 합성이나 영화 후편집 분야에서 확산 모델 활용 가능성을 시사합니다. 더 나아가, BIVDiff와 같은 방법은 훈련 없이 기존 이미지 확산 모델과 일반 텍스트-투-비디오 모델을 결합해 임의의 영상 생성을 시도하기도 했습니다 ￼. 이처럼 확산 모델을 2D 이미지에서 3D/비디오로 확장하는 흐름은 계속 강해질 것으로 보이며, 뒤에서 다룰 3D 비전 분야와도 맞물려 있습니다.

요약하면, 생성적 AI의 핵심 기술로서 확산 모델은 지난 1년간 더 좋게, 더 정확하게, 더 빠르게 만들기 위한 다각도의 혁신이 이루어졌습니다. 이러한 발전은 이미지 생성 품질의 새로운 SOTA 달성뿐만 아니라, 정교한 컨트롤 (예: 다중객체 배치), 실시간 응용 가능성 (추론 가속), **다른 도메인 확장(영상, 3D)**이라는 성과로 나타났습니다. 이는 곧 콘텐츠 제작 자동화, 디지털 트윈 및 VR/AR 콘텐츠 생성, 크리에이티브 도구 개발 등 산업 현장에서의 적용 가능성을 높이고 있습니다.

멀티모달 융합과 비전-언어 모델의 트렌드

**거대 언어 모델(LLM)**의 시대 속에서, 비전-언어 멀티모달 모델 역시 지난 1년간 괄목할 만한 발전을 이루었습니다. 특히 텍스트와 이미지를 동시에 이해하거나 생성할 수 있는 이들 모델은 멀티모달 AI의 핵심으로 부상했습니다. 그러나 한편으로 LLM에 비해 VLM(Vision-Language Model)의 발전 속도가 더딘 원인에 대한 분석과 돌파가 중요한 주제로 떠올랐습니다 ￼. CVPR 2024에서 발표된 InternVL 연구는 이러한 문제의식을 바탕으로 한 대표적 성과입니다. 이 논문은 **“비전 모델이 LLM만큼 규모와 학습량이 크지 못했고, 비전 표현과 언어 표현의 연결이 비효율적”**이라는 진단을 내리고 ￼, 이에 대한 해법으로 비전 모델의 대폭 스케일 업과 효과적 정렬을 제시했습니다. 구체적으로, 기존 CLIP 등으로 대표되는 비전-언어 모델이 수억~수천만 규모의 비전 백본을 쓰던 것과 달리, **InternVL은 약 60억 파라미터짜리 거대 비전 트랜스포머(ViT-6B)**를 구축했습니다 ￼. 그리고 이 거대 비전 모델을 대용량 이미지-텍스트 데이터로 대비학습하고, 이후 LLM과 결합하여 멀티모달 이해 및 생성 태스크에 투입했습니다 ￼. 특히 LLM과의 접합부에는 Q-Former 스타일의 가벼운 어댑터 대신, LLM의 중간 계층과 Vision Encoder를 직접 연동하는 등 밀결합된 구조를 사용했습니다 ￼. 그 결과 InternVL은 이미지 분류, 객체 탐지/분할 같은 전통 비전 과제부터, zero-shot 이미지/비디오 분류, 이미지-텍스트 검색, 비전+언어 대화에 이르기까지 32개의 다양한 벤치마크에서 기존 SOTA를 새로 썼습니다 ￼. 이는 멀티모달 기초 모델로서 InternVL이 갖는 범용성을 보여주는 것으로, 사실상 비전 분야의 GPT-3/4에 준하는 대형 모델이 등장했음을 의미합니다. 특히 InternVL-G 모델은 후속 미세조정을 통해 비디오 이해, VQA, 캡셔닝 등 여러 영역에서 최고 수준을 기록하여, 멀티모달 AI의 새로운 성능 한계를 제시했습니다 ￼. 이러한 성과는 **“비전 모델도 크고 충분히 데이터가 주어지면 언어 모델에 필적하는 일반 능력을 가질 수 있다”**는 것을 입증하고, 차후 더 거대한 ViT-22B 등으로의 발전 가능성도 언급하고 있습니다 ￼.

멀티모달 모델의 발전에서 모델의 성능 평가와 신뢰성 문제도 중요한 이슈로 부상했습니다. 기존에도 VQA, 이미지 캡셔닝 등의 벤치마크가 있었지만, LLM 시대에 걸맞은 복합적인 평가가 필요해졌습니다. 이에 2024년 등장한 MMMU (Massive Multi-discipline Multimodal Understanding) 벤치마크는 30개 학문영역, 183개 세부 분야에 걸친 방대한 시각-언어 이해 평가를 제시했습니다 ￼. MMMU는 단순 이미지 묘사나 질문응답을 넘어, 표, 그래프, 화학 구조식 등 다양한 종류의 시각 정보를 포함하고, 추론 능력까지 측정하도록 설계되었습니다 ￼. 이는 현재 멀티모달 모델들이 단순 암기적 패턴 인식은 잘하지만 고차원적 추론은 미흡하다는 지적에 대응하려는 움직임입니다. 실제로 InternVL 등의 모델도 이러한 포괄적인 벤치마크에서 완벽한 성능과는 거리가 있는데, 향후 멀티모달 모델의 지능 향상 방향을 제시해주는 셈입니다.

또 다른 트렌드로, 멀티모달 모델의 모달리티 확장을 들 수 있습니다. 기존에는 이미지-텍스트 2모달이 주류였지만, 2024년에는 이미지-텍스트-오디오-영상-심지어 센서 신호까지 통합하려는 시도가 나오고 있습니다. 예를 들어 Microsoft의 Kosmos-2 모델은 이미지와 언어 외에 영역(bounding box)과 포인팅 개념을 도입하여, 문장에서 언급된 객체를 이미지 상에 지목하거나 참조하는 능력을 강화했습니다. 이는 시각적 그라운딩 능력을 통해 사람과 AI의 상호작용을 자연스럽게 만들려는 방향입니다. 또한 Meta AI가 공개한 **ImageBind (2023)**는 이미지, 소리, 텍스트, 깊이, 열화상 등의 6개 모달리티를 단일 임베딩 공간에 묶어, 예를 들어 음향을 입력받아 그에 맞는 이미지 생성이나 이미지를 보고 관련 소리 검색 같은 새로운 멀티모달 응용을 가능케 했습니다. 이러한 연구들은 인간의 오감에 해당하는 데이터를 함께 이해하는 범용 AI로 가는 초석이라 볼 수 있습니다. 2025년에는 이러한 멀티모달 결합이 영상 및 로봇 제어로까지 확장되어, 예컨대 로봇에게 시각 정보를 주고 언어로 명령하여 작업 수행을 평가하는 Embodied AI 과제들(EQA 등)도 등장하고 있습니다 ￼. 이는 비전과 언어, 그리고 행동이 통합된 총체적 AI 시스템으로의 진화를 보여주는 흐름입니다.

거대 멀티모달 모델의 경량화와 확산도 중요한 화두입니다. 거대 모델은 성능은 높지만 활용 장벽이 높기에, 산업계에서는 경량 멀티모달 모델에 대한 수요가 있습니다. 2024년 CVPR에서는 Meta가 제안한 MobileSAM(EfficientSAM)이라는 모델이 소개되었는데, 이는 2023년 큰 반향을 일으킨 **Segment Anything Model (SAM)**을 모바일에서도 실행 가능하도록 경량화한 버전입니다 ￼. EfficientSAM은 대규모 마스크 데이터로 사전학습된 SAM의 성능을 최대한 유지하면서도 매개변수 수와 연산량을 크게 줄인 모델들로, 기존 대비 수십 배 빠르게 동작해 실시간 세그멘테이션을 가능케 합니다 ￼. 이러한 노력은 멀티모달 모델 또한 경량화 전략(지식 증류, 구조최적화 등)을 통해 실무 배치가 가능함을 보여줍니다. 비전-언어 모델에 대해서도 MobileCLIP 등이 시도되었는데, 이는 CLIP 모델을 강화학습을 접목한 훈련으로 작은 모델에서도 유사한 이미지-텍스트 매칭 성능을 내도록 한 것입니다 ￼. 결과적으로, 거대 비전-언어 모델의 지식을 작은 모델에 증류하거나 특정 용도에 맞게 최적화하는 연구가 병행되어, 클라우드부터 엣지 기기까지 멀티모달 AI 활용이 가능해지고 있습니다.

요약하면, 멀티모달 비전-언어 분야에서는 **“크게 만들고, 잘 평가하고, 넓게 확장하고, 작게도 만들어보자”**라는 네 가지 축으로 트렌드가 전개되고 있습니다. InternVL 같은 거대 모델은 멀티모달 AI의 새로운 한계를 제시했고, MMMU 등의 벤치마크는 모델의 한계점을 드러내며 도전과제를 주었습니다. 동시에 모달리티의 추가와 융합을 통해 더 인간과 같은 인지 체계로 발전하는 한편, 경량화로 현실 세계에 스며드는 기술로 자리잡고 있습니다. 이러한 흐름은 장기적으로 시각과 언어, 나아가 여러 감각을 모두 이해하고 소통하는 AI, 즉 AGI에 가까운 멀티모달 에이전트의 등장을 향해 가속화되고 있습니다.

3D 비전과 NeRF 기술의 최신 동향

3D 인식과 생성은 컴퓨터 비전 분야에서 지속적으로 중요한 테마이며, **NeRF(Neural Radiance Field)**를 필두로 한 신경 렌더링 기법이 등장한 이후로 격변을 겪고 있습니다. 2024-2025년에는 NeRF의 성능 및 효율 향상, 동적인 3D 장면 처리, 단일 이미지의 3D 이해 및 복원, 확산 모델의 3D 적용 등이 두드러진 방향입니다.

먼저, 다중 뷰로부터의 고품질 3D 재구성에서 NeRF 계열 모델들의 품질은 이미 상당히 높았지만, 효율성과 일반화 측면에서 도전이 남아있었습니다. 이를 해결하기 위해 **3D Gaussian Splatting(3D-GS)**과 같은 명시적 표현 기반 NeRF 변형들이 연구되었는데, 이는 방대한 신경망 대신 가우시안 요소들로 씬을 표현하여 학습과 렌더링을 가속하는 방법입니다. 2024년에는 이를 시간 차원까지 확장한 4D Gaussian Splatting 기법이 소개되어, 동영상 같은 동적 장면을 효율적으로 배울 수 있게 했습니다 ￼. 예를 들어, 매 프레임마다 별도로 3D-GS를 적용하던 방식을 넘어 시간 축을 포함한 통합 표현으로 저장함으로써 일관된 동영상 NeRF를 구현한 것입니다. 이는 동영상 기반 3D 재구성이나 가상/증강현실 콘텐츠 생성에 유용하며, 메모리와 계산도 절약합니다 ￼.

동적 3D 장면에 대한 이해와 생성도 중요한 주제입니다. CVPR 2024 베스트 페이퍼 중 하나인 Generative Image Dynamics는 단일 정지화상으로부터 움직이는 동영상을 생성하는 모델로, 진폭-주파수 표현을 예측하여 진동하는 자연현상(나무 흔들림, 물결 등)을 합성했습니다 ￼ ￼. 이 모델은 기본적으로 확산 모델을 사용하되, 출력으로 스펙트럼 볼륨을 예측하여 시간축을 주파수 도메인으로 표현하는 독특한 접근을 취했습니다 ￼. 이러한 시도는 정적 이미지→동적 시뮬레이션이라는 새로운 형태의 3D+시간 콘텐츠 생성을 보여줍니다. 나아가 **PhysGaussian (CVPR 2024)**이라는 연구는 앞서 언급한 Gaussian Splatting에 연속체 역학(physics)을 통합하여, 물리적으로 개연성 있는 3D 동작을 생성하는 방안을 제시했습니다 ￼. 이 방법은 중간에 메쉬로 변환하지 않고도 3D 가우시안 표현 자체를 물리 시뮬레이션 (예: 재료점법, MPM)을 통해 움직이게 만들어, 계단을 오르는 사람 등의 동작을 현실감 있게 복원했습니다 ￼ ￼. 이는 모션 합성과 3D 재구성을 결합한 흥미로운 결과로서, 향후 모든 것이 연결된 4D 세계 모델 쪽으로의 가능성을 열었습니다.

단일 이미지로부터 3D 정보를 복원하는 문제도 큰 진전을 보였습니다. 전통적으로 단일 사진으로 3D를 추출하는 것은 정보 부족으로 매우 어려운 문제였지만, 대규모 학습과 제약 도입으로 성과가 나오고 있습니다. 위 표의 **Wonder3D (CVPR 2024)**는 단일 이미지에서 텍스처까지 갖춘 고품질 3D 메쉬를 뽑아내는데, 교차 도메인 확산 모델을 활용한 점이 특징입니다. 이 모델은 우선 주어진 이미지와 뷰 각도를 조금씩 달리한 가상 이미지들에 대해, 해당 장면의 노말맵(법선 맵)을 생성합니다 ￼. 쉽게 말해, 여러 시점에서 본 표면 기울기 정보를 추측해내는 것입니다. 그리고 나서 이렇게 얻은 다중 노말맵들을 geometry-aware normal fusion이라는 알고리즘으로 통합하여, 정합성이 높은 3D 형상을 재구성합니다 ￼. 마지막으로 텍스처까지 투영하여, 거의 사진 같은 정밀도를 가진 3D 모델을 단 한 장의 입력으로 만들어냅니다. 놀라운 것은 이 모든 과정이 2~3분 내로 완료될 정도로 최적화되었다는 점으로, 이는 기존 단일 영상 3D 복원 기법보다 훨씬 효율적입니다 ￼. 성능 지표상으로도 기하학적 세부 묘사와 정확도 면에서 SOTA에 도달했다고 하며, 즉각적인 AR/VR 콘텐츠 생성이나 전문가 없는 3D 모델링 등에 응용 가능성이 높습니다.

또 다른 각도로, NeRF를 활용한 새로운 응용들도 등장했습니다. **ZeroNVS (CVPR 2024)**는 앞서 언급했듯 단일 이미지로 360도 뷰를 합성하는 혁신적 모델입니다. 이 작업은 입력 이미지에 보이지 않는 시야까지 상상하여 만들어야 하므로 상당한 추론이 필요한데, ZeroNVS는 확산 모델+NeRF의 형태로 이를 풀었습니다. 핵심은 카메라 포즈를 3자유도(방향+거리)에서 6자유도(방향+거리+평행이동)로 확장하여 복잡한 장면에서도 개체들의 위치 변화를 정확히 반영하고 ￼ ￼, Score Distillation Sampling(SDS) 기반으로 훈련하면서 배경 다양성 문제를 해결한 것입니다 ￼ ￼. SDS 훈련만 쓰면 모드 붕괴로 모든 배경이 똑같이 나와버릴 수 있는데, DDIM기반으로 미리 다양한 배경을 생성해 앵커로 삼는 ‘SDS Anchoring’ 기법을 고안해 이 문제를 완화했습니다 ￼ ￼. 그 결과 ZeroNVS는 실내부터 실외까지 다양한 실제 장면에서 단일 이미지 입력만으로도 일관되고 현실감 있는 3D 회전 영상을 만들어내어, 기존 기법들을 능가하는 새로운 SOTA를 달성했습니다 ￼. 이는 사진 한 장으로 3D를 볼 수 있게 하는 기술로, 부동산 VR 투어, 문화유산 복원, 일반 사용자용 3D 사진 등 많은 응용이 기대됩니다.

이벤트 카메라와 NeRF를 결합한 연구도 흥미로운 사례입니다. 이벤트 카메라는 미세한 밝기 변화만을 고속으로 측정하는 센서로, 초당 수천 프레임의 변화를 기록할 수 있어 모션 블러가 없는 장점이 있습니다. CVPR 2024의 한 연구에서는 이벤트 카메라 신호와 일반 카메라 프레임을 동시에 활용하여 모션 블러를 줄인 NeRF 재구성을 선보였습니다 ￼ ￼. 이는 야간이나 고속 움직임 환경에서 더 정확한 3D 재구성을 가능케 해주며, 자율주행 등에서 3D 인식을 향상시킬 수 있습니다.

전반적으로 3D 비전 분야의 최근 동향은 더 정확하고 현실적인 3D 획득과 더 빠른 학습 및 렌더링 두 측면으로 요약됩니다. NeRF와 그 변형들은 이제 실시간성에 근접하고 있고, 씬의 크기나 동적 요소 처리도 점점 가능해지고 있습니다 ￼. 특히 2024년 CVPR에서 NeRF 및 3DGS 관련 논문이 120편 이상 발표되었다고 보고될 정도로 ￼, 이 분야 연구가 폭발적으로 증가했습니다. 이는 곧 VR/AR, 로봇비전, 메타버스 등 3D 정보가 핵심인 응용 분야에서 딥러닝 기반 기법들이 실제로 활용될 날이 가까워지고 있음을 시사합니다. 앞으로는 확산 모델과 3D 생성의 결합, 멀티모달 3D (텍스트로 3D 생성 등), 대규모 3D 사전학습 모델 등이 등장하여, 3D 비전에서도 기초모델화가 진행될 전망입니다 ￼ ￼. 요컨대 3D 비전은 이제 정적 재구성 단계를 넘어 동적, 확률적 생성 단계로 진화하고 있으며, 현실 세계의 디지털 트윈을 구현하는 방향으로 나아가고 있습니다.

효율적인 비전 아키텍처와 모델 경량화

컴퓨터 비전 모델의 대형화 추세 속에서도, 효율성은 여전히 중요한 연구 화두입니다. 2024-2025년 사이에는 모델 구조 자체의 효율적 설계, 특수 연산자의 최적화, 대형 모델의 경량화 기법 등이 주목받았습니다. 이는 실제 제품/서비스에 SOTA 모델을 탑재하기 위한 필수 연구로서, 모바일/임베디드 기기에서의 실시간 동작, 에너지 소모 절감, 추론 비용 감소 등을 목표로 합니다.

먼저, **비전 트랜스포머(ViT)**와 ConvNet의 장점을 결합하면서 더 빠른 속도/정확도를 양립하려는 시도가 이어졌습니다. 그 중 **EfficientViM (CVPR 2025)**은 최근 눈에 띄는 성과로, **State-Space Model(SSM)**이라는 새로운 시퀀스 처리 기법을 도입한 혼합형 비전 백본입니다 ￼. 기존에는 지역 정보는 합성곱으로, 전역 정보는 어텐션으로 처리하는 경량 모델들이 많았는데, EfficientViM은 여기에 선형 복잡도의 SSM을 추가해 전역 의존성을 더욱 효율적으로 포착합니다 ￼ ￼. 특히 **Hidden State Mixer 기반 SSD 레이어(HSM-SSD)**를 고안하여, SSM의 내부 연산을 채널 축으로 압축된 숨은 상태 상에서 수행함으로써 메모리 대역폭 병목을 줄이고 속도를 높였습니다 ￼. 또한 여러 단계의 숨은 상태를 융합(fusion)하여 표현력을 확보했지요 ￼. 그 결과 EfficientViM 모델군은 ImageNet-1k 이미지 분류에서 이전까지 최고로 효율적이던 모델(SHViT 등)을 0.7%p 상회하는 정확도를 더 빠른 추론속도로 달성했습니다 ￼. 이미지 해상도를 키우거나 지식 증류를 활용하는 등 확장 시나리오에서도 이전 경량 비전 트랜스포머들보다 처리량(throughput)과 정확도에서 모두 향상을 보였다고 합니다 ￼. 이는 경량 모델의 새로운 SOTA로 평가되며, EfficientViM의 아이디어는 향후 임베디드 비전 모델 설계에 적용될 것으로 기대됩니다.

컨볼루션 연산자 차원의 혁신으로는, **Deformable Convolution v4 (DCNv4)**가 대표적입니다. 변형 합성곱은 ConvNet에 어텐션 유연성을 도입한 강력한 연산자로 알려져 있는데, 2022년 InternImage 등에서 쓰인 DCNv3의 속도 한계를 극복하기 위해 DCNv4가 개발되었습니다. DCNv4는 공간 가중치 결합 시 Softmax 정규화를 제거하여 동적인 범위를 확보했고, 구현 측면에서 메모리 접근을 최적화하여 불필요한 연산을 줄였습니다 ￼ ￼. 그 결과 이전 버전 대비 3배 이상의 속도를 보이며, 일반적인 비전 연산자들(예: 어텐션, Depthwise Conv 등)보다도 빠른 수준을 달성했습니다 ￼ ￼. 또한 동일한 네트워크에서 학습 시 수렴 속도가 크게 개선되어, 적은 이터레이션으로도 높은 정확도를 얻었습니다 ￼ ￼. DCNv4를 Latent Diffusion 모델의 U-Net 등에 통합하면 기존 대비 향상된 이미지 생성 품질을 보였고, InternImage 백본에서 DCNv3를 v4로 대체하기만 해도 추가 튜닝 없이 최대 80%의 추론속도 향상과 정확도 상승이 있었습니다 ￼ ￼. 이는 ConvNet 계열 모델의 부활을 이끄는 중요한 성과로, 복잡한 어텐션 없이도 ConvNet을 최적화하면 여전히 경쟁력 있는 정확도와 훨씬 뛰어난 효율을 낼 수 있음을 보여줍니다 ￼. 실제로 ResNet 계열이나 최신 ConvNext 기반 모델에 DCNv4를 적용한 FlashInternImage 등의 시도가 이어지고 있으며, 고해상도 영상 처리나 실시간 검출 등 분야에서 각광받고 있습니다.

대형 모델의 경량화 역시 실용화 측면에서 중요합니다. 앞서 EfficientSAM, MobileCLIP 등을 예로 들었지만, 이외에도 모델 압축, 양자화, 지식증류 기법들이 비전 분야 SOTA 모델들에 적용되고 있습니다. 예를 들어, 2024년 등장한 몇몇 연구들은 거대 ViT 모델을 4비트 양자화나 저항도 프루닝 등을 통해 성능 하락 없이 모델 크기를 절반 이하로 줄이는 결과를 보이기도 했습니다. 또한 특정 도메인 특화 모델 경량화도 이뤄지고 있는데, 의료영상용 비전 트랜스포머를 가볍게 만든 SegFormer3D나 ￼, 위성영상 도로추출용 경량 세그멘테이션 등 다양한 사례가 나왔습니다. 이러한 Task-specific한 효율 모델은 클라우드 자원에 접근이 어려운 환경이나 실시간성 요구가 높은 현장에서 쓰일 수 있어 주목받고 있습니다.

마지막으로, **네트워크 아키텍처 검색(NAS)**이나 AutoML 기법을 통해 주어진 제약 내 최적 구조를 찾는 연구도 지속되고 있습니다. 특히 비전 트랜스포머 구조를 자동 탐색하여 모바일 GPU에서 최고 성능/속도를 내는 구조를 찾거나 ￼, 여러 센서를 조합한 멀티모달 모델을 효율적으로 구성하는 NAS도 등장했습니다. 이는 모델러가 일일이 설계하지 않아도 주어진 하드웨어나 목적에 꼭 맞는 모델을 설계할 수 있게 해주며, 향후 엣지 디바이스별 맞춤형 비전 모델 시대를 열어갈 기술입니다.

정리하면, 효율적 비전 아키텍처 분야에서는 **새로운 연산자와 구조의 개발(EfficientViM, DCNv4 등)**과 **거대 모델의 실용화 전략(경량화, NAS 등)**이 중심을 이루고 있습니다. 이는 최신 SOTA 모델을 실제 제품에 활용하기 위한 필수 조건으로서, 업계와 학계 모두 큰 관심을 갖는 부분입니다. 자율주행차의 제한된 컴퓨팅 환경, 모바일 앱의 실시간 AR 기능, 대규모 CCTV 분석의 비용 절감 등 현실 문제들을 생각하면, 이러한 효율화 연구의 중요성은 더욱 부각됩니다. 향후에는 파운드리 차원의 AI 가속기와 공조한 모델 설계, 메모리-계산 일체화 모델 등의 방향으로 더 혁신적인 효율성 향상이 이루어질 것으로 예상됩니다.

###결론 및 전망

2024년 하반기부터 2025년 현재까지의 컴퓨터 비전 분야 동향을 살펴보면, 한마디로 **“기술적 융합과 규모 확장, 그리고 현실 적용을 위한 최적화”**로 요약할 수 있습니다. 생성 모델의 혁신(특히 확산 모델)은 이미지, 동영상, 3D 생성의 질적 수준을 끌어올렸고, 멀티모달 모델의 발전은 시각적 지능과 언어적 지능의 결합을 통해 보다 범용적인 AI로 진화하고 있습니다. 동시에 3D 비전은 학습된 모델이 현실 세계를 더 입체적이고 동적으로 이해하도록 하고 있으며, 효율적인 아키텍처 연구는 이러한 최첨단 모델들이 현실 장비와 애플리케이션 위에서 원활히 돌아가도록 뒷받침하고 있습니다.

흥미로운 점은 이 각각의 트렌드들이 서로 교차하며 새로운 시너지를 낸다는 것입니다. 예를 들어 멀티모달 확산 모델은 텍스트 지시로 3D 객체를 생성하거나 ￼, 비전-언어 모델이 3D 장면을 서술하게 하는 등, 한 분야의 성과가 다른 분야의 발전을 촉진하고 있습니다. 또한 모든 분야에서 대규모 데이터와 모델의 힘을 실감하면서도, 동시에 책임감 있는 AI(신뢰성, 평가)와 현실성(속도, 경량화)을 고려하는 성숙함이 드러납니다. 이는 컴퓨터 비전 기술이 이제 단순한 인식 정확도 경쟁을 넘어, 실제 세계에 통합되는 단계로 접어들었음을 나타냅니다.

향후 1~2년간 우리는 비전 분야의 더욱 통합된 기반 모델(예: 이미지+언어+3D+행동을 아우르는 모델), 인간 수준의 실시간 시각지능(복잡한 장면에서의 즉각적 이해와 생성), 그리고 경량이면서도 똑똑한 에지 비전 AI의 등장을 기대할 수 있습니다. 그 기반에는 본 문서에서 다룬 확산 모델의 지속적 개선, 멀티모달 융합, 3D 표현의 진화, 아키텍처 효율화가 자리하게 될 것입니다. 컴퓨터 비전의 최첨단은 계속 변화하고 있지만, 결국 목표는 인간과 같이 보고 이해하고 창작하는 인공지능으로 수렴하고 있습니다. 현재의 SOTA 기술들이 그 목표에 한 걸음씩 다가가는 주춧돌 역할을 하고 있으며, 이러한 추세는 당분간 지속될 것으로 전망됩니다.

참고문헌: 해당 내용 작성에 인용된 주요 문헌 및 자료들은 본문 각주에 명시되어 있습니다. 주요 학회 논문(CVPR, ICCV, ECCV 등)과 아카이브 논문에서 발췌한 정보 ￼ ￼, 업계 기술 블로그 ￼ 등을 종합하여 기술했습니다. 각각의 출처는 본문 내 각주 링크를 통해 확인할 수 있습니다. 특히 본 보고서에 언급된 논문들의 원문과 저자 제공 코드/프로젝트 페이지는 표의 “관련 링크”에 제공되어 있으므로, 심도있는 검토를 원하시는 분들은 해당 링크를 참고하시기 바랍니다.  ￼ ￼