{"text": "[SEED] synthetic text #0. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #1. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #2. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #3. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #4. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #5. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #6. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #7. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #8. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #9. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #10. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #11. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #12. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #13. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #14. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #15. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #16. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #17. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #18. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #19. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #20. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #21. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #22. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #23. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #24. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #25. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #26. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #27. This is a simulated paper summary about agents and policies.", "label": 1}
{"text": "[SEED] synthetic text #28. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "[SEED] synthetic text #29. This is a simulated paper summary about agents and policies.", "label": 0}
{"text": "이 논문 요약은 유익했다", "label": 1}
{"text": "이 논문 요약은 유익했다", "label": 1}
{"text": "내용이 어렵고 이해하기 힘들었다", "label": 0}
{"text": "요약이 부정확하고 도움이 안 됐다", "label": 0}
{"text": "품질이 낮고 실용성이 없다", "label": 0}
{"text": "핵심을 잘 짚어줘서 이해가 쉬웠다", "label": 1}
{"text": "정리 품질이 높고 실무에 도움이 된다", "label": 1}
{"text": "내용이 어렵고 이해하기 힘들었다", "label": 0}
{"text": "요약이 부정확하고 도움이 안 됐다", "label": 0}
{"text": "품질이 낮고 실용성이 없다", "label": 0}
{"text": "핵심을 잘 짚어줘서 이해가 쉬웠다", "label": 1}
{"text": "정리 품질이 높고 실무에 도움이 된다", "label": 1}
{"text": "정확하고 유익했다", "label": 1}
{"text": "도움이 안 된다", "label": 0}
{"text": "정확하고 유익했다", "label": 1}
{"text": "정확하고 유익했다", "label": 1}
{"text": "정확하고 유익했다", "label": 1}
{"text": "정확하고 유익했다", "label": 1}
{"text": "내용이 어렵고 이해하기 힘들었다", "label": 0}
{"text": "요약이 부정확하고 도움이 안 됐다", "label": 0}
{"text": "핵심을 잘 짚어줘서 이해가 쉬웠다", "label": 1}
{"text": "정리 품질이 높고 실무에 도움이 된다", "label": 1}
{"title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview", "summary": "We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04450v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset", "summary": "We present ChronoGraph, a graph-structured multivariate time series\nforecasting dataset built from real-world production microservices. Each node\nis a service that emits a multivariate stream of system-level performance\nmetrics, capturing CPU, memory, and network usage patterns, while directed\nedges encode dependencies between services. The primary task is forecasting\nfuture values of these signals at the service level. In addition, ChronoGraph\nprovides expert-annotated incident windows as anomaly labels, enabling\nevaluation of anomaly detection methods and assessment of forecast robustness\nduring operational disruptions. Compared to existing benchmarks from industrial\ncontrol systems or traffic and air-quality domains, ChronoGraph uniquely\ncombines (i) multivariate time series, (ii) an explicit, machine-readable\ndependency graph, and (iii) anomaly labels aligned with real incidents. We\nreport baseline results spanning forecasting models, pretrained time-series\nfoundation models, and standard anomaly detectors. ChronoGraph offers a\nrealistic benchmark for studying structure-aware forecasting and incident-aware\nevaluation in microservice systems.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04449v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "Towards Cognitively-Faithful Decision-Making Models to Improve AI Alignment", "summary": "Recent AI work trends towards incorporating human-centric objectives, with\nthe explicit goal of aligning AI models to personal preferences and societal\nvalues. Using standard preference elicitation methods, researchers and\npractitioners build models of human decisions and judgments, which are then\nused to align AI behavior with that of humans. However, models commonly used in\nsuch elicitation processes often do not capture the true cognitive processes of\nhuman decision making, such as when people use heuristics to simplify\ninformation associated with a decision problem. As a result, models learned\nfrom people's decisions often do not align with their cognitive processes, and\ncan not be used to validate the learning framework for generalization to other\ndecision-making tasks. To address this limitation, we take an axiomatic\napproach to learning cognitively faithful decision processes from pairwise\ncomparisons. Building on the vast literature characterizing the cognitive\nprocesses that contribute to human decision-making, and recent work\ncharacterizing such processes in pairwise comparison tasks, we define a class\nof models in which individual features are first processed and compared across\nalternatives, and then the processed features are then aggregated via a fixed\nrule, such as the Bradley-Terry rule. This structured processing of information\nensures such models are realistic and feasible candidates to represent\nunderlying human decision-making processes. We demonstrate the efficacy of this\nmodeling approach in learning interpretable models of human decision making in\na kidney allocation task, and show that our proposed models match or surpass\nthe accuracy of prior models of human pairwise decision-making.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04445v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data", "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04443v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "Delta Activations: A Representation for Finetuned Large Language Models", "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04442v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04441v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "Low-rank matrix and tensor approximations: advancing efficiency of machine-learning interatomic potentials", "summary": "Machine-learning interatomic potentials (MLIPs) have become a mainstay in\ncomputationally-guided materials science, surpassing traditional force fields\ndue to their flexible functional form and superior accuracy in reproducing\nphysical properties of materials. This flexibility is achieved through\nmathematically-rigorous basis sets that describe interatomic interactions\nwithin a local atomic environment. The number of parameters in these basis sets\ninfluences both the size of the training dataset required and the computational\nspeed of the MLIP. Consequently, compressing MLIPs by reducing the number of\nparameters is a promising route to more efficient simulations. In this work, we\nuse low-rank matrix and tensor factorizations under fixed-rank constraints to\nachieve this compression. In addition, we demonstrate that an algorithm with\nautomatic rank augmentation helps to find a deeper local minimum of the fitted\npotential. The methodology is verified using the Moment Tensor Potential (MTP)\nmodel and benchmarked on multi-component systems: a Mo-Nb-Ta-W medium-entropy\nalloy, molten LiF-NaF-KF, and a glycine molecular crystal. The proposed\napproach achieves up to 50% compression without any loss of MTP accuracy and\ncan be applied to compress other MLIPs.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04440v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory", "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04439v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform", "summary": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04437v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
{"title": "Time, quantum entanglement, and particle decay", "summary": "We investigate the role of time ordering in entanglement experiments\ninvolving unstable particles, focusing on $\\mu^+ \\mu^-$ pairs produced in a\nmaximally-entangled spin state. We analyse the correlations between\nmeasurements performed by two experimenters, Alice (who measures $\\mu^-$ spin)\nand Bob (who measures $\\mu^+$ decay products). Remarkably, the observed\ncorrelations persist irrespective of whether Bob's muon decays before or after\nAlice's spin measurement. We further discuss different interpretations of the\nsame empirical results depending on the observer's reference frame. The\nfindings reinforce the viewpoint that the Copenhagen interpretation of\nmeasurement is a mathematical tool rather than a literal account of physical\nreality.", "authors": [], "pdf_url": "http://arxiv.org/pdf/2509.04436v1", "source": "collector_v2", "query": "reinforcement learning", "idea": "강화학습 에이전트의 탐험 전략 개선", "keywords": ["Reinforcement Learning", "Exploration", "Novelty"], "label": 1}
